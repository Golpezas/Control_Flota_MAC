import pandas as pd
from pymongo import MongoClient
from datetime import datetime
import os
import re
import numpy as np 
from typing import Dict, List, Any

# [CORRECCI√ìN INICIAL] Configuraci√≥n para silenciar el FutureWarning de downcasting en Pandas
pd.set_option('future.no_silent_downcasting', True)

# =========================================================================
# 1. CONFIGURACI√ìN
# =========================================================================
# Importante: Reemplaza tu contrase√±a aqu√≠.
DB_PASSWORD = "VhNG9h2rfXAy2xxv" 
CONNECTION_STRING = f"mongodb+srv://antoniohernandezmm_db_user:{DB_PASSWORD}@flotacluster.yipfgjz.mongodb.net/?retryWrites=true&w=majority&appName=FlotaCluster"
DB_NAME = 'MacSeguridadFlota'
CSV_FOLDER = 'Archivos_CSV'
DOC_ROOT_FOLDER = 'MOVILES' 
CED_VERDE_ROOT = 'Ced Verdes' 

# Formatos de fecha comunes para una conversi√≥n robusta
DATE_FORMATS = [
    '%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d',  
    '%d/%m/%y', '%d-%m-%y',              
    '%d/%m/%Y %H:%M:%S', '%Y/%m/%d %H:%M:%S', 
    '%d.%m.%Y',                          
    '%d/%m' 
]

# =========================================================================
# 2. FUNCIONES AUXILIARES DE LIMPIEZA Y CONVERSI√ìN
# =========================================================================

def cleanup_dataframe_for_mongo(df: pd.DataFrame) -> pd.DataFrame:
    """
    [CORRECCI√ìN DEFINITIVA DE NaT] 
    Convierte NaT (Pandas Not a Time) y NaN (Not a Number) a None de Python. 
    Asegura que las columnas de fecha sean de tipo 'object' con objetos Python nativos 
    para la serializaci√≥n de MongoDB.
    """
    print("  [LOG-LIMPIEZA] Aplicando saneamiento final de NaT/NaN a None y forzando tipos nativos.")
    
    df_cleaned = df.copy()

    # 1. Aplicar .mask(pd.isna, None) para convertir todos los nulos (NaN, NaT) a Python None.
    df_cleaned = df_cleaned.mask(pd.isna, None)
    
    # 2. [L√çNEA CR√çTICA] Forzar columnas de fecha/tiempo a dtype 'object'.
    # Esto garantiza que PyMongo solo vea datetime.datetime o None de Python.
    for col in df_cleaned.select_dtypes(include=['datetime64[ns]']).columns:
        # Se aplica nuevamente la conversi√≥n, pero esta vez se fuerza el dtype 'object'.
        df_cleaned[col] = df_cleaned[col].apply(lambda x: x.to_pydatetime() if pd.notna(x) else None).astype(object)
        
    return df_cleaned


def clean_column_key(key: Any) -> str:
    """Limpia y estandariza las claves de columna."""
    key = str(key).lstrip('\ufeff').lstrip('√è¬ª¬ø')
    key = key.upper().strip()
    key = re.sub(r'[^\w\s]', '', key) 
    key = re.sub(r'\s+', '_', key)
    return key

def rename_and_filter(df: pd.DataFrame, column_map: Dict[str, str]) -> pd.DataFrame:
    """Renombra columnas de un DataFrame bas√°ndose en un mapa limpio."""
    cleaned_map = {clean_column_key(k): v for k, v in column_map.items()}
    # Renombra solo las columnas que existen en el DataFrame
    return df.rename(columns={k: v for k, v in cleaned_map.items() if k in df.columns})

def safe_date_convert(series: pd.Series) -> pd.Series:
    """
    Convierte una Serie de Pandas a datetime, manejando m√∫ltiples formatos.
    Retorna una Serie de 'object' dtype con datetime.datetime o None de Python.
    """
    
    # 1. Limpieza de strings de valores faltantes
    series_cleaned = series.astype(str).str.upper().str.strip()
    missing_value_strings = ['NAN', 'NONE', 'N/A', 'SIN FECHA', '', 'NO', 'NA'] 
    series_cleaned = series_cleaned.replace(missing_value_strings, np.nan) 
    
    # 2. Intento inicial con `dayfirst=True`
    dates = pd.to_datetime(series_cleaned, errors='coerce', dayfirst=True)

    # 3. L√≥gica de reintento para formatos espec√≠ficos
    if dates.isna().sum() > len(series_cleaned) / 2: 
        for fmt in DATE_FORMATS:
            failed_indices = dates[dates.isna()].index
            if failed_indices.empty:
                break
            try:
                # El warning sobre la ambig√ºedad del a√±o es de Python, no de Pandas o del c√≥digo
                converted_part = pd.to_datetime(series_cleaned.loc[failed_indices], format=fmt, errors='coerce')
                dates.loc[failed_indices] = converted_part.loc[failed_indices]
            except ValueError:
                continue 

    # 4. Convierte NaT a None, y Timestamp a datetime nativo de Python, forzando dtype object
    # El retorno es compatible con PyMongo.
    return dates.apply(lambda x: x.to_pydatetime() if pd.notna(x) else None).astype(object)

def safe_currency_convert(series: pd.Series) -> pd.Series:
    """
    Convierte una Serie de formato moneda latino/argentino a un valor num√©rico float.
    """
    if series.empty:
        return series
    
    original_series = series.astype(str)
    
    # 1. Limpieza: Eliminar caracteres no num√©ricos, punto o coma.
    s = original_series.str.replace(r'[^\d\.,]', '', regex=True).str.strip()

    # 2. Identificar valores en formato Latam (Ej: 84.137,58)
    latam_mask = s.str.contains(r'\.') & s.str.contains(r',')

    # 3. Procesar valores Latam: Remover miles (punto) y cambiar coma por punto decimal.
    s.loc[latam_mask] = s.loc[latam_mask].str.replace('.', '', regex=False)
    s.loc[latam_mask] = s.loc[latam_mask].str.replace(',', '.', regex=False)

    # 4. Procesar valores con solo coma: cambiar coma por punto decimal (cubre casos como 420,00)
    s.loc[~latam_mask & s.str.contains(r',')] = s.loc[~latam_mask & s.str.contains(r',')].str.replace(',', '.', regex=False)

    # 5. Convertir a num√©rico, forzando NaNs para fallos
    numeric_series = pd.to_numeric(s, errors='coerce')
    
    # üîé LOG DE CONTROL
    log_index = latam_mask.idxmax() if latam_mask.any() else None
    if log_index is not None and log_index in series.index:
        original_val = original_series.loc[log_index]
        final_val = numeric_series.loc[log_index]
        print(f"  [LOG-MONEDA] ‚úÖ Conversi√≥n corregida (Ejemplo): '{original_val.strip()}' -> {final_val}")

    # 6. Rellenar NaNs con 0 y redondear
    return numeric_series.fillna(0).round(2)


# =========================================================================
# 3. FUNCI√ìN PRINCIPAL DE NORMALIZACI√ìN
# =========================================================================

def process_and_normalize_data() -> Dict[str, List[Dict[str, Any]]]:
    """Lee, limpia, normaliza todos los CSVs y prepara los datos para la carga."""
    print("Iniciando proceso de normalizaci√≥n de todos los CSVs...")
    
    # Inicializaci√≥n de DataFrames
    df_vehiculos = pd.DataFrame()
    df_documentacion = pd.DataFrame()
    df_mantenimiento = pd.DataFrame()
    df_infracciones = pd.DataFrame()
    df_componentes = pd.DataFrame()
    df_flota_estado = pd.DataFrame()
    
    # --- UTILITY FUNCTION: Leer CSV de forma robusta ---
    def load_csv(filename: str) -> pd.DataFrame:
        path = os.path.join(CSV_FOLDER, filename)
        
        print(f"  [DEBUG] Buscando archivo en ruta: {path}")

        if not os.path.exists(path):
            print(f"‚ùå ERROR: Archivo {filename} NO ENCONTRADO.")
            return pd.DataFrame()
        
        try:
            # 1. Intentamos leer con 'utf-8-sig' y luego 'latin-1' para manejar codificaci√≥n
            df = pd.read_csv(path, encoding='utf-8-sig', sep=None, engine='python', on_bad_lines='skip')
            if df.empty or any('\ufeff' in col for col in df.columns):
                 df = pd.read_csv(path, encoding='latin-1', sep=None, engine='python', on_bad_lines='skip')
            
            if df.empty:
                print(f"  [DEBUG] {filename} cargado, pero el DataFrame est√° VAC√çO. (0 filas)")
                return pd.DataFrame()
            
            print(f"  [DEBUG] ‚úÖ {filename} cargado con {len(df)} filas. Iniciando limpieza...")
            
            # 2. Limpieza de encabezados
            cleaned_columns = []
            for col in df.columns:
                col_str = str(col).strip().lstrip('\ufeff').lstrip('√è¬ª¬ø')
                try:
                    col_str = col_str.encode('latin-1').decode('utf-8', 'ignore')
                except:
                    pass
                col_str = re.sub(r'[.$:\(\)]', '', col_str)
                col_str = re.sub(r'\s+', '_', col_str).upper()
                cleaned_columns.append(col_str)

            df.columns = cleaned_columns
            
            # 3. Renombrar las columnas de patente/dominio a la clave est√°ndar: 'PATENTE'
            df.rename(columns={
                'PATENTE_': 'PATENTE', 'DOMINIO': 'PATENTE', 
                'DOMINIO_': 'PATENTE', 'PATENTES': 'PATENTE' 
            }, inplace=True)
            
            return df
        except Exception as e:
            print(f"‚ùå ERROR CR√çTICO al cargar {filename}: {e}")
            return pd.DataFrame()


    # =====================================================================
    # A) DOCUMENTACION Y MAESTRO DE VEH√çCULOS 
    # =====================================================================
    
    df_doc = load_csv('documentacion.csv') 
    
    if not df_doc.empty:
        print(f"  [LOG-DIAG 1] Columnas detectadas en documentacion.csv DESPU√âS de limpieza: {list(df_doc.columns)}") 
        if 'PATENTE' in df_doc.columns:
            print("  [LOG-DIAG 1.1] ‚úÖ Columna 'PATENTE' estandarizada y detectada.")
        else:
            print("  [LOG-DIAG 1.1] ‚ùå Columna 'PATENTE' estandarizada NO detectada.")
        
    if not df_doc.empty and 'PATENTE' in df_doc.columns: 
        print("-> Procesando documentacion.csv (Maestro)...")
        
        doc_map = {
            'PATENTE': '_id', 'NRO_MOVIL': 'nro_movil', 'TIPO_DE_COMB': 'tipo_combustible',
            'MOVIL': 'descripcion_modelo', 'MODELO': 'anio', 'COLOR': 'color',
            'MEDIDAS_DE_CUBIERTAS': 'medidas_cubiertas', 'CLAVE_RADIO': 'clave_radio',
            'ASEGURADORA': 'aseguradora' 
        }
        
        df_doc_clean = rename_and_filter(df_doc, doc_map)
        
        if '_id' in df_doc_clean.columns:
            df_doc_clean['_id'] = df_doc_clean['_id'].astype(str).str.upper().str.strip()
            
            # 3. CONSOLIDACI√ìN DE VEH√çCULOS (df_vehiculos)
            cols_vehiculos = ['_id', 'nro_movil', 'tipo_combustible', 'descripcion_modelo', 'anio', 'color', 'medidas_cubiertas', 'clave_radio']
            df_vehiculos = df_doc_clean[[col for col in cols_vehiculos if col in df_doc_clean.columns]].drop_duplicates(subset=['_id']).copy()
            df_vehiculos['activo'] = True
            
            print(f"  [LOG-DIAG 3] Filas finales de df_vehiculos (Maestro) despu√©s de consolidaci√≥n: {len(df_vehiculos)}")

            # 4. GENERAR DOCUMENTACI√ìN (df_documentacion)
            vencimiento_map = [('VENCIMIENTO_CEDULA', 'Cedula'), ('VENCIMIENTO_SEGURO', 'Seguro'), ('VENC_GAS', 'GNC'), ('VTV', 'VTV'), ('TARJ_YPF', 'TARJ YPF')]
            
            for index, row in df_doc_clean.iterrows():
                patente = row['_id']
                
                for original_col, doc_type in vencimiento_map:
                    col_name_cleaned = clean_column_key(original_col) 
                    
                    if col_name_cleaned in df_doc.columns and pd.notna(df_doc.loc[df_doc.index[index], col_name_cleaned]):
                        
                        df_documentacion = pd.concat([df_documentacion, pd.DataFrame([{
                            'patente': patente, 
                            'tipo_documento': doc_type, 
                            'fecha_vencimiento': df_doc.loc[df_doc.index[index], col_name_cleaned], 
                            'aseguradora': row.get('aseguradora') 
                        }])], ignore_index=True)

    # 4.1 CONVERSI√ìN DE FECHAS DE DOCUMENTACI√ìN
    if not df_documentacion.empty and 'fecha_vencimiento' in df_documentacion.columns:
        df_documentacion['fecha_vencimiento'] = safe_date_convert(df_documentacion['fecha_vencimiento'])


    # =====================================================================
    # Z) REFERENCIAS A ARCHIVOS DIGITALES
    # =====================================================================
    
    if not df_vehiculos.empty:
        print("-> Generando referencias a 3 archivos digitales: Titulo, Poliza y Cedula Verde...")
        df_vehiculos['documentos_digitales'] = pd.Series([None] * len(df_vehiculos), index=df_vehiculos.index, dtype=object)
        
        for index, row in df_vehiculos.iterrows():
            patente = row['_id']
            modelo_base = str(row.get('descripcion_modelo', '')).split(' ')[0].strip().upper()
            folder_name_old = f"{modelo_base} {patente}".strip()
            document_list = []
            
            titulo_path = os.path.join(DOC_ROOT_FOLDER, folder_name_old, 'TITULO AUTOMOTOR.pdf').replace('\\', '/')
            document_list.append({'tipo': 'TITULO_AUTOMOTOR', 'nombre_archivo': 'TITULO AUTOMOTOR.pdf', 'path_esperado': titulo_path})
        
            poliza_pattern = 'Poliza*' 
            poliza_path_ejemplo = os.path.join(DOC_ROOT_FOLDER, folder_name_old, 'Poliza (Fecha Variable).pdf').replace('\\', '/')
            document_list.append({'tipo': 'POLIZA_SEGURO_DIGITAL', 'nombre_archivo_patron': poliza_pattern, 'path_patron_ejemplo': poliza_path_ejemplo})
            
            ced_verde_filename = f"{patente}.jpg"
            ced_verde_path = os.path.join(CED_VERDE_ROOT, ced_verde_filename).replace('\\', '/')
            document_list.append({'tipo': 'CEDULA_VERDE_DIGITAL', 'nombre_archivo': ced_verde_filename, 'path_esperado': ced_verde_path})
            
            df_vehiculos.at[index, 'documentos_digitales'] = document_list


    # =====================================================================
    # B) POLIZAS Y BAJAS 
    # =====================================================================
    
    df_polizas = load_csv('polizas.csv')
    if not df_polizas.empty and 'PATENTE' in df_polizas.columns:
        print("-> Procesando polizas.csv...")
        poliza_map = {'PATENTE': 'patente', 'SUMA_ASEGURADA': 'suma_asegurada', 'COSTO_SEMESTRAL': 'costo_semestral', 'COSTO_MENSUAL': 'costo_mensual', 'MONTO_FRANQ': 'monto_franquicia'}
        df_polizas_clean = rename_and_filter(df_polizas, poliza_map)
        
        if 'patente' in df_polizas_clean.columns:
            df_polizas_clean['patente'] = df_polizas_clean['patente'].astype(str).str.upper().str.strip()
            df_polizas_clean['tipo_documento'] = 'Poliza_Detalle'
            
            # Conversi√≥n de campos monetarios
            currency_cols = ['suma_asegurada', 'costo_semestral', 'costo_mensual', 'monto_franquicia']
            for col in currency_cols:
                if col in df_polizas_clean.columns:
                    df_polizas_clean[col] = safe_currency_convert(df_polizas_clean[col])
            
            cols_to_use = [col for col in ['patente', 'tipo_documento', 'suma_asegurada', 'costo_semestral', 'costo_mensual', 'monto_franquicia'] if col in df_polizas_clean.columns]
            df_documentacion = pd.concat([df_documentacion, df_polizas_clean[cols_to_use]], ignore_index=True)


    df_bajas = load_csv('vendidos_o_bajas.csv')
    if not df_bajas.empty and 'PATENTE' in df_bajas.columns:
        print("-> Procesando vendidos_o_bajas.csv (Flota_Estado)...")
        bajas_map = {'PATENTE': 'patente', 'DENUNCIA_DE_VENTA': 'fecha_estado', 'TRANSFERENCIA_08': 'motivo_estado_transferencia', 'OTROS': 'motivo_estado_otro'}
        df_bajas_clean = rename_and_filter(df_bajas, bajas_map)
        if 'patente' in df_bajas_clean.columns:
            df_bajas_clean['patente'] = df_bajas_clean['patente'].astype(str).str.upper().str.strip()
            patentes_baja = df_bajas_clean['patente'].unique()
            if not df_vehiculos.empty:
                df_vehiculos.loc[df_vehiculos['_id'].isin(patentes_baja), 'activo'] = False
            df_bajas_clean['estado'] = 'Baja'
            df_bajas_clean['tipo'] = 'BAJA_DEFINITIVA'
            cols_to_use = [col for col in ['patente', 'fecha_estado', 'motivo_estado_transferencia', 'motivo_estado_otro', 'estado', 'tipo'] if col in df_bajas_clean.columns]
            df_flota_estado = pd.concat([df_flota_estado, df_bajas_clean[cols_to_use]], ignore_index=True)

    if not df_flota_estado.empty and 'fecha_estado' in df_flota_estado.columns:
        df_flota_estado['fecha_estado'] = safe_date_convert(df_flota_estado['fecha_estado'])


    # =====================================================================
    # C) MANTENIMIENTO
    # =====================================================================

    mantenimiento_files_map = {
        'servicios_renault.csv': {'tipo': 'SERVICIO_RENAULT', 'map': {'FECHA': 'fecha', 'KMS': 'kilometraje_km', 'MOTIVO': 'motivo', 'DESCRIPCION': 'descripcion', 'LUGAR': 'lugar', 'MONTO': 'costo_monto', 'FACTURA_NRO': 'factura_nro'}},
        'servicios_lavallol.csv': {'tipo': 'SERVICIO_LAVALLOL', 'map': {'FECHA': 'fecha', 'KMS': 'kilometraje_km', 'MOTIVO': 'motivo', 'DESCRIPCION': 'descripcion'}},
        'reparaciones.csv': {'tipo': 'REPARACION_EXTERNA', 'map': {'FECHA': 'fecha', 'KILOMETRAJE': 'kilometraje_km', 'MOTIVO': 'motivo', 'LUGAR': 'lugar'}},
        'taller_2024.csv': {'tipo': 'TALLER_MOVIL', 'map': {'FECHA': 'fecha', 'MOTIVO': 'motivo', 'MOVIL_N¬∫': 'nro_movil'}},
        'taller_2025.csv': {'tipo': 'TALLER_MOVIL', 'map': {'FECHA': 'fecha', 'MOTIVO': 'motivo', 'MOVIL_N¬∫': 'nro_movil'}},
    }
    
    for filename, data in mantenimiento_files_map.items():
        df = load_csv(filename)
        
        if not df.empty and 'PATENTE' in df.columns: 
            print(f"-> Procesando {filename}...")
        
            df.rename(columns={'PATENTE': 'patente'}, inplace=True)
            df_clean = rename_and_filter(df, data['map'])
            df_clean['tipo_registro'] = data['tipo']
            
            if 'patente' in df_clean.columns:
                df_clean['patente'] = df_clean['patente'].astype(str).str.upper().str.strip()
            
            if 'kilometraje_km' in df_clean.columns:
                 # Redondear y convertir a entero (Int64 para manejar NaNs)
                 df_clean['kilometraje_km'] = pd.to_numeric(df_clean['kilometraje_km'], errors='coerce').round(0).astype('Int64')
            
            # Aplicar safe_currency_convert
            if 'costo_monto' in df_clean.columns:
                 df_clean['costo_monto'] = safe_currency_convert(df_clean['costo_monto'])
                 
            df_mantenimiento = pd.concat([df_mantenimiento, df_clean], ignore_index=True)

    moviles_files = ['moviles_octubre.csv', 'moviles_septiembre.csv']
    for filename in moviles_files:
          df = load_csv(filename)
          if not df.empty and 'PATENTE' in df.columns:
              print(f"-> Procesando {filename} (Kilometraje)...")
              movil_map = {'PATENTE': 'patente', 'PROX_SERV_KM_ACEITE_Y_FILTROS': 'prox_serv_km'}
              df.rename(columns={'PATENTE': 'patente'}, inplace=True)
              df_clean = rename_and_filter(df, movil_map)
              df_clean['tipo_registro'] = 'CONTROL_KM_SERVICIO'
              if 'patente' in df_clean.columns:
                  df_clean['patente'] = df_clean['patente'].astype(str).str.upper().str.strip()
              cols_to_use = [col for col in ['patente', 'prox_serv_km', 'tipo_registro', 'OBSERVACIONES'] if col in df_clean.columns]
              df_mantenimiento = pd.concat([df_mantenimiento, df_clean[cols_to_use]], ignore_index=True)

    if not df_mantenimiento.empty and 'fecha' in df_mantenimiento.columns:
        df_mantenimiento['fecha'] = safe_date_convert(df_mantenimiento['fecha'])


    # =====================================================================
    # D) INFRACCIONES Y MULTAS (Finanzas)
    # =====================================================================
    
    infracciones_files_map = {
        'infracciones_caba.csv': {'map': {'DIA': 'dia', 'A√ëO': 'a√±o', 'IMPORTE': 'monto', 'FALTA': 'motivo', 'LUGAR': 'lugar', 'DATOS_CONDUCTOR': 'conductor', 'DATOS_ACOMPA√ëANTE': 'acompanante'}},
        'infracciones_ezeiza.csv': {'map': {'FECHA_INFRACCI√ìN': 'fecha_infraccion', 'MONTO': 'monto', 'MOTIVO': 'motivo', 'LUGAR_INFRACCI√ìN': 'lugar'}},
        'infracciones_florencio_varela.csv': {'map': {'FECHA_DE_OCURRENCIA': 'fecha_infraccion', 'IMPORTE__': 'monto', 'FALTA': 'motivo', 'LUGAR_DE_OCURRENCIA': 'lugar'}},
        'infracciones_zamora.csv': {'map': {'FECHA_INFRACCI√ìN': 'fecha_infraccion', 'IMPORTE': 'monto', 'FALTA': 'motivo', 'LUGAR': 'lugar'}},
        'multas_prov_bs_as.csv': {'map': {'FECHA_DE_OCURRENCIA': 'fecha_infraccion', 'IMPORTE__': 'monto', 'FALTA': 'motivo', 'LUGAR_DE_OCURRENCIA': 'lugar'}},
    }
    
    for filename, data in infracciones_files_map.items():
        df = load_csv(filename)
        
        if not df.empty and 'PATENTE' in df.columns: 
            print(f"-> Procesando {filename} (Infracciones)...")
            
            df.rename(columns={'PATENTE': 'patente'}, inplace=True)
            df_clean = rename_and_filter(df, data['map'])
            
            # L√≥gica especial para Infracciones CABA que usa 'DIA' y 'A√ëO'
            if filename == 'infracciones_caba.csv':
                if 'dia' in df_clean.columns and 'a√±o' in df_clean.columns:
                    # Concatenar para formar una fecha legible (ej: 01/2025)
                    df_clean['fecha_infraccion'] = df_clean['dia'].astype(str).str.cat(df_clean['a√±o'].astype(str), sep='/')
                    df_clean.drop(columns=['dia', 'a√±o'], errors='ignore', inplace=True)
            
            df_clean['tipo_registro'] = 'INFRACCION'
            df_clean['jurisdiccion'] = filename.replace('.csv', '').replace('infracciones_', '').replace('multas_prov_bs_as', 'BS_AS').upper()
            
            if 'patente' in df_clean.columns:
                 df_clean['patente'] = df_clean['patente'].astype(str).str.upper().str.strip()
            
            # Aplicar safe_currency_convert
            if 'monto' in df_clean.columns:
                 df_clean['monto'] = safe_currency_convert(df_clean['monto'])

            df_infracciones = pd.concat([df_infracciones, df_clean], ignore_index=True)

    if not df_infracciones.empty and 'fecha_infraccion' in df_infracciones.columns:
        df_infracciones['fecha_infraccion'] = safe_date_convert(df_infracciones['fecha_infraccion'])


    # =====================================================================
    # E) COMPONENTES (Baterias & Neum√°ticos)
    # =====================================================================
    
    componentes_files = {
        'baterias_neumaticos.csv': 'PATENTE',
        'alaskan.csv': 'PATENTE',
    }
    
    comp_cols = {
        'REEMPLAZO_NEUMATICOS_DELANTEROS': 'Neumatico_Delantero',
        'REEMPLAZO_NUEMATICOS_TRASEROS': 'Neumatico_Trasero',
        'REEMPLAZO_BATERIAS': 'Bateria',
    }
    
    kms_col_clean = clean_column_key('KMS')

    for filename, pk_col in componentes_files.items():
        df = load_csv(filename)
        
        if not df.empty and 'PATENTE' in df.columns:
            print(f"-> Procesando {filename} (Componentes)...")
            
            df.rename(columns={'PATENTE': 'patente'}, inplace=True)
            df['patente'] = df['patente'].astype(str).str.upper().str.strip()
            
            for index, row in df.iterrows():
                patente = row['patente']
                
                for original_col_uncleaned, tipo_componente in comp_cols.items():
                    col_name = clean_column_key(original_col_uncleaned) 
                    
                    if col_name in row and pd.notna(row[col_name]):
                        kms_value = row[kms_col_clean] if kms_col_clean in row else None
                        
                        kilometraje_final = None
                        if pd.notna(kms_value): # Usar pd.notna para todos los tipos de nulos
                            # Se usa round(0) y conversi√≥n a Int64
                            try:
                                kilometraje_final = pd.to_numeric(kms_value, errors='coerce').round(0).astype('Int64').iloc[0]
                            except:
                                kilometraje_final = None

                        df_componentes = pd.concat([df_componentes, pd.DataFrame([{
                            'patente': patente, 
                            'tipo_componente': tipo_componente, 
                            'fecha_instalacion': row[col_name],
                            'kilometraje_instalacion': kilometraje_final
                        }])], ignore_index=True)

    if not df_componentes.empty and 'fecha_instalacion' in df_componentes.columns:
        df_componentes['fecha_instalacion'] = safe_date_convert(df_componentes['fecha_instalacion'])


    # =====================================================================
    # F) CONSOLIDACI√ìN FINAL (Bloque CORREGIDO)
    # =====================================================================
    
    print("\n‚úîÔ∏è Normalizaci√≥n completa. Consolidando colecciones...")
    
    print(f"  [DEBUG_FINAL] df_vehiculos tiene {len(df_vehiculos)} registros.")
    print(f"  [DEBUG_FINAL] df_documentacion tiene {len(df_documentacion)} registros.")
    print(f"  [DEBUG_FINAL] df_mantenimiento tiene {len(df_mantenimiento)} registros.")
    print(f"  [DEBUG_FINAL] df_infracciones tiene {len(df_infracciones)} registros.")
    print(f"  [DEBUG_FINAL] df_componentes tiene {len(df_componentes)} registros.")
    print(f"  [DEBUG_FINAL] df_flota_estado tiene {len(df_flota_estado)} registros.")

    # Aplicamos cleanup_dataframe_for_mongo (con el FIX de NaT) a CADA DataFrame
    normalized_data = {
        'Vehiculos': cleanup_dataframe_for_mongo(df_vehiculos).to_dict('records') if not df_vehiculos.empty else [],
        'Documentacion': cleanup_dataframe_for_mongo(df_documentacion).to_dict('records') if not df_documentacion.empty else [],
        'Mantenimiento': cleanup_dataframe_for_mongo(df_mantenimiento).to_dict('records') if not df_mantenimiento.empty else [],
        'Finanzas': cleanup_dataframe_for_mongo(df_infracciones).to_dict('records') if not df_infracciones.empty else [],
        'Componentes': cleanup_dataframe_for_mongo(df_componentes).to_dict('records') if not df_componentes.empty else [],
        'Flota_Estado': cleanup_dataframe_for_mongo(df_flota_estado).to_dict('records') if not df_flota_estado.empty else [],
    }

    return normalized_data

# =========================================================================
# 4. FUNCI√ìN DE CARGA (PyMongo)
# =========================================================================

def load_data_to_mongodb(data: Dict[str, List[Dict[str, Any]]]):
    """Establece la conexi√≥n e inserta los datos en las colecciones."""
    
    client = None
    try:
        print("\nüåê Intentando conectar con MongoDB Atlas...")
        client = MongoClient(CONNECTION_STRING)
        client.admin.command('ping') 
        db = client[DB_NAME]
        print(f"‚úÖ Conexi√≥n exitosa a la base de datos: {DB_NAME}")
        
        from pymongo import UpdateOne 

        for collection_name, records in data.items():
            if not records:
                print(f"‚ö†Ô∏è Saltando colecci√≥n '{collection_name}': No hay registros para insertar.")
                continue

            collection = db[collection_name]
            
            if collection_name == 'Vehiculos':
                print(f"\n‚öôÔ∏è Procesando colecci√≥n '{collection_name}' ({len(records)} registros)...")
                
                updates = [
                    (
                        {'_id': record['_id']}, 
                        {'$set': record},       
                        True                    
                    ) for record in records if '_id' in record
                ]
                
                if updates:
                    bulk_operations = [
                        UpdateOne(filter_doc, update_doc, upsert_flag) 
                        for filter_doc, update_doc, upsert_flag in updates
                    ]
                    collection.bulk_write(bulk_operations, ordered=False)
                    print(f"‚úÖ Colecci√≥n '{collection_name}' actualizada con √©xito (usando bulk_write).")
                else:
                    print(f"‚ö†Ô∏è Colecci√≥n '{collection_name}' sin registros v√°lidos para actualizaci√≥n.")
            else:
                # El resto de colecciones se borran y se vuelven a insertar.
                collection.drop() 
                collection.insert_many(records)
                print(f"‚úÖ Colecci√≥n '{collection_name}' insertada con √©xito: {len(records)} documentos.")

    except Exception as e:
        print(f"‚ùå ERROR CR√çTICO durante la carga a MongoDB: {e}")
        print("Aseg√∫rate de que la 'CONNECTION_STRING' y la contrase√±a sean correctas.")
    finally:
        if client:
            client.close()
            print("Conexi√≥n a MongoDB cerrada.")

# =========================================================================
# 5. FUNCI√ìN PRINCIPAL
# =========================================================================

def main():
    print("--- INICIO DEL PROCESO ETL MULTI-CSV ---")
    
    normalized_data = process_and_normalize_data()
    
    if normalized_data:
        load_data_to_mongodb(normalized_data)
    
    print("\n--- PROCESO ETL FINALIZADO ---")

if __name__ == '__main__':
    main()